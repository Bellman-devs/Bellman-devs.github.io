Search.setIndex({docnames:["_autosummary/bellman","_autosummary/bellman.agents","_autosummary/bellman.agents.background_planning","_autosummary/bellman.agents.decision_time_planning","_autosummary/bellman.agents.mbpo","_autosummary/bellman.agents.mepo","_autosummary/bellman.agents.pets","_autosummary/bellman.agents.trpo","_autosummary/bellman.benchmark","_autosummary/bellman.benchmark.mbpo","_autosummary/bellman.benchmark.mepo","_autosummary/bellman.benchmark.pets","_autosummary/bellman.benchmark.trpo","_autosummary/bellman.distributions","_autosummary/bellman.drivers","_autosummary/bellman.environments","_autosummary/bellman.environments.mixins","_autosummary/bellman.environments.mixins.BatchSizeUpdaterMixin","_autosummary/bellman.environments.transition_model","_autosummary/bellman.environments.transition_model.keras_model","_autosummary/bellman.harness","_autosummary/bellman.networks","_autosummary/bellman.policies","_autosummary/bellman.training","_autosummary/bellman.training.utils","_autosummary/bellman.trajectory_optimisers","index","notebooks/approximate_mdps","notebooks/model_visualisation","notebooks/trajectory_optimisation"],envversion:{"sphinx.domains.c":2,"sphinx.domains.changeset":1,"sphinx.domains.citation":1,"sphinx.domains.cpp":4,"sphinx.domains.index":1,"sphinx.domains.javascript":2,"sphinx.domains.math":2,"sphinx.domains.python":3,"sphinx.domains.rst":2,"sphinx.domains.std":2,"sphinx.ext.intersphinx":1,"sphinx.ext.viewcode":1,nbsphinx:3,sphinx:56},filenames:["_autosummary/bellman.rst","_autosummary/bellman.agents.rst","_autosummary/bellman.agents.background_planning.rst","_autosummary/bellman.agents.decision_time_planning.rst","_autosummary/bellman.agents.mbpo.rst","_autosummary/bellman.agents.mepo.rst","_autosummary/bellman.agents.pets.rst","_autosummary/bellman.agents.trpo.rst","_autosummary/bellman.benchmark.rst","_autosummary/bellman.benchmark.mbpo.rst","_autosummary/bellman.benchmark.mepo.rst","_autosummary/bellman.benchmark.pets.rst","_autosummary/bellman.benchmark.trpo.rst","_autosummary/bellman.distributions.rst","_autosummary/bellman.drivers.rst","_autosummary/bellman.environments.rst","_autosummary/bellman.environments.mixins.rst","_autosummary/bellman.environments.mixins.BatchSizeUpdaterMixin.rst","_autosummary/bellman.environments.transition_model.rst","_autosummary/bellman.environments.transition_model.keras_model.rst","_autosummary/bellman.harness.rst","_autosummary/bellman.networks.rst","_autosummary/bellman.policies.rst","_autosummary/bellman.training.rst","_autosummary/bellman.training.utils.rst","_autosummary/bellman.trajectory_optimisers.rst","index.rst","notebooks/approximate_mdps.ipynb","notebooks/model_visualisation.ipynb","notebooks/trajectory_optimisation.ipynb"],objects:{"":{bellman:[0,0,0,"-"]},"bellman.agents":{background_planning:[2,0,0,"-"],decision_time_planning:[3,0,0,"-"],mbpo:[4,0,0,"-"],mepo:[5,0,0,"-"],pets:[6,0,0,"-"],trpo:[7,0,0,"-"]},"bellman.benchmark":{mbpo:[9,0,0,"-"],mepo:[10,0,0,"-"],pets:[11,0,0,"-"],trpo:[12,0,0,"-"]},"bellman.environments":{mixins:[16,0,0,"-"],transition_model:[18,0,0,"-"]},"bellman.environments.mixins":{BatchSizeUpdaterMixin:[17,1,1,""]},"bellman.environments.mixins.BatchSizeUpdaterMixin":{update_batch_size:[17,2,1,""]},"bellman.environments.transition_model":{keras_model:[19,0,0,"-"]},"bellman.training":{utils:[24,0,0,"-"]},bellman:{agents:[1,0,0,"-"],benchmark:[8,0,0,"-"],distributions:[13,0,0,"-"],drivers:[14,0,0,"-"],environments:[15,0,0,"-"],harness:[20,0,0,"-"],networks:[21,0,0,"-"],policies:[22,0,0,"-"],training:[23,0,0,"-"],trajectory_optimisers:[25,0,0,"-"]}},objnames:{"0":["py","module","Python module"],"1":["py","class","Python class"],"2":["py","method","Python method"]},objtypes:{"0":"py:module","1":"py:class","2":"py:method"},terms:{"0":28,"05":28,"1":[27,28,29],"10":28,"100":28,"1000":28,"10000":28,"1000000":28,"11":28,"12":28,"16":28,"17":28,"18":28,"1814195236":28,"19":28,"1e":28,"2":28,"20":28,"200":28,"25000":28,"256":28,"258358337":28,"2607481859":28,"2824442390":28,"3":28,"3286094786":28,"34":28,"3602052060":28,"39":28,"3903815623":28,"3989627936":28,"4":28,"4200779551":28,"4204429102":28,"493832024":28,"5":28,"5000":28,"6":28,"64":28,"7":28,"8":28,"9":28,"99":28,"abstract":17,"case":27,"class":[16,17,18,23,27,29],"function":[8,13,27,28],"import":28,"int":17,"new":17,"return":[17,28],"true":27,A:[27,29],For:[15,25,28],In:[23,27,28],No:28,That:26,The:[0,14,23,26,27,28,29],There:29,These:[28,29],To:0,_:28,a_i:29,a_t:27,abc:17,about:27,action:[14,27,28,29],action_spec:28,adamoptim:28,add_batch:28,after:28,again:28,against:27,agent:[0,8,14,20,23,24,26,27,29],agenttrain:23,aim:0,algorithm:[27,29],all:[27,28],along:28,also:27,altern:26,an:[14,23,26,27,28,29],api:[0,26],appli:29,applic:27,approxim:[15,26,29],ar:[22,27,28,29],as_dataset:28,assembl:28,associ:28,axi:28,background:2,base:[1,17,22,23,26,27,28,29],batch:[17,28],batch_siz:[17,28],becaus:28,been:[8,27,28,29],befor:28,bellman:[27,28],between:27,blue:28,bound:28,build:27,c:27,call:[14,27,28],callback:28,can:[27,28,29],capabl:26,captur:28,car:28,cdot:27,check:0,choos:[27,29],chosen:27,classic_control:28,clear:28,collect:28,collect_data_spec:28,collect_driv:28,collect_model_training_episod:28,collect_polici:28,collect_steps_per_iter:28,colour:28,combin:29,common:[27,28],compat:28,compon:[16,17,23,27],compos:[27,29],computation:27,conceiv:26,condit:[28,29],consid:27,consist:[23,27,28],constantreward:28,constrain:29,contain:[18,19,21,22,28],control:28,cross:29,cumul:27,current:29,data:[23,28],data_spec:28,dataset:28,debug_summari:28,decis:[3,26,27,28,29],deep:27,def:28,defin:[14,15,16,23,25,27,28],definit:27,defint:27,demonstr:28,describ:27,design:27,detail:[15,27],determinist:27,differ:[27,29],directli:27,discount:27,discret:28,distribut:[27,28],document:[0,27],done:27,dqn:28,dqn_agent:28,dqnagent:28,driver:28,durat:28,dynam:27,dynamic_episode_driv:28,dynamic_step_driv:28,dynamicepisodedriv:28,dynamicstepdriv:28,each:[23,27,29],earlystop:28,easili:27,effici:27,element:28,element_wise_squared_loss:28,enabl:26,end:27,enough:28,ensembl:27,ensur:27,entir:27,entropi:29,enumer:23,env:27,environ:[14,23,28,29],environment_model:28,environmentmodel:[27,28],episod:[27,28],epoch:28,epsilon:28,epsilon_greedi:28,estim:29,eval_polici:28,exampl:[23,28],execut:29,expect:27,expens:27,experi:[8,20,28],extend:26,extens:29,f:27,f_i:27,factor:27,fals:28,fc_layer_param:28,flexibl:[27,29],follow:27,form:27,framework:[27,29],free:[23,26],from:[14,23,27,29],fulfil:27,gamma:[27,28,29],gather:28,gather_al:28,gaussian:27,gener:[27,28],get:0,get_initial_st:28,get_or_create_global_step:28,given:29,global_step:28,go:28,gradient_clip:28,greedi:28,green:28,gt:28,gym:[27,28],h:29,ha:[23,27,28],hand:27,handl:27,hat:[27,29],have:[8,23,27,28,29],helper:[8,23],horizon:27,how:[27,28],howev:27,hyper:29,i:29,implement:[8,18,19,21,23,27],improv:28,impul:28,impuls:28,incorpor:27,independ:29,infinit:27,infinitehorizontrajectorysampl:28,inform:[25,28],infti:27,inifit:27,initi:[26,27,28,29],initial_state_distribut:28,inlin:28,instead:27,interact:14,interfac:[18,23,27,28],intern:17,involv:27,ipykernel_2487:28,iter:[28,29],jupyt:26,kera:[19,21,28],keras_model:28,kerastrainingspec:28,kerastransitionmodel:28,kernel:27,last:28,layer:21,learn:[1,22,26,27,29],learning_r:28,left:28,length:[28,29],level:0,librari:[26,27],linear:28,linear_transition_network:28,linearis:28,lineartransitionnetwork:28,list:23,load:28,locat:28,loss:28,lt:28,m:[27,29],made:28,mai:[23,27,29],manag:14,manipul:13,manner:[27,28],mapsto:27,markov:27,mathbb:27,mathcal:[27,29],matplotlib:28,max_length:28,mbrl:26,mdp:[15,26,29],mean:28,method:[17,23,25,29],mirror:0,model:[1,15,16,17,18,19,21,22,23,26,29],model_collect_driv:28,model_training_buff:28,model_training_buffer_capac:28,modul:[2,3,16,18,19,28],modular:27,modulenotfounderror:28,monitor:28,more:[15,23,25,27],most:[27,28],mountain:28,mountain_car:28,mountaincar:28,mountaincarinitialst:28,mountaincarreward:28,mountaincartermin:28,must:27,n_step_upd:28,name:[23,28],nameerror:28,necessari:27,network:[19,27,28],neural:[19,27,28],next:28,next_observ:28,none:[17,28],note:27,notebook:[15,25,26,28],now:28,num_episod:28,num_parallel_cal:28,num_step:28,number:[28,29],numpi:28,object:[23,27,29],observ:28,observation_spec:28,often:27,one:[23,27,28],open:26,openai:27,optim:[28,29],optimis:25,order:28,other:27,out:0,overview:26,p:[27,29],packag:[0,1,8,13,14,15,20,21,22,23,25,29],paramet:[17,23,29],parameteris:[23,29],parametris:28,part:[17,26],particular:27,patienc:28,pi:29,plan:[2,3,29],plot:28,plot_mountain_car_policy_decis:28,plot_mountain_car_transit:28,polici:[14,23,28,29],policy_st:28,policytrajectoryoptimis:29,posit:28,posterior:27,predict:[26,28],prefetch:28,previou:28,probabilist:27,probabl:13,problem:[26,29],process:[27,29],produc:[28,29],provid:[1,8,13,20,23,27,29],py:28,python:[26,27],q:28,q_net:28,q_network:28,qnetwork:28,r:[27,29],random:[28,29],rang:28,re:27,read:26,real:[15,23,28,29],recent:28,red:28,refer:[0,26],reinforc:[1,22,26,27,28],repeat:[27,29],repeatedli:29,replac:27,replay_buff:28,replay_buffer_capac:28,repositori:27,repres:28,resampl:27,respect:27,respons:14,reward:[27,28,29],reward_model:28,reward_scale_factor:28,rho_0:[27,29],right:28,rl:[8,14,23,26],role:27,rollout:[27,28,29],run:[8,20,28],s:[14,27,29],s_0:29,s_:[27,29],s_i:29,s_t:27,same:[27,28],sampl:[14,27,29],sample_batch_s:28,sample_trajectori:28,sample_transit:28,sample_uniformly_distributed_observations_and_get_act:28,sample_uniformly_distributed_transit:28,schedul:23,see:[15,25],sequenc:29,sequenti:26,set:[23,29],shoot:29,should:23,sim:27,singl:[27,28,29],size:17,small:28,so:27,solut:29,solv:[26,27,29],some:29,sourc:[17,26],space:[27,28],specif:22,specifi:23,start:[0,27,29],state:[17,27,28,29],statesampl:27,step:[14,27,29],stochast:27,straightforward:28,strategi:27,structur:0,subclass:27,subsequ:28,suite_gym:28,sum:29,summarize_grads_and_var:28,superclass:27,support:[24,27],swap:27,system:[27,28],t:27,tackl:26,target_update_period:28,target_update_tau:28,td_errors_loss_fn:28,tensorflow:[26,27,28],termin:[27,28],test:28,tf:[0,23,27],tf_agent:28,tf_env:28,tf_py_environ:28,tf_uniform_replay_buff:28,tf_wrapper:28,tfpyenviron:28,tftimelimit:28,tfuniformreplaybuff:28,than:23,thi:[1,8,13,14,15,16,18,19,20,21,22,23,25,27,28,29],those:[23,29],three:28,time:[3,27,29],time_step:28,time_step_spec:28,tmp:28,tool:28,toolbox:[0,8,23,26,27,29],top:0,traceback:28,train_loss:28,train_model_step:28,train_sequence_length:28,train_step:28,train_step_count:28,trainabl:23,training_batch_s:28,training_spec:28,trajectori:[25,27,28],trajectory_optimis:29,trajectory_sampl:28,trajectory_sampling_strategi:28,trajectory_spec:28,transit:[18,19,21,23,28,29],transition_model:28,transitionmodel:27,transtion:27,trial:27,ts1:27,ts:27,tupl:27,two:[27,28,29],type:[17,27],uniformli:27,unrol:29,until:27,untrain:28,updat:23,update_batch_s:17,us:[17,19,21,23,27,28,29],usual:27,util:28,v0:28,v1:28,valu:[17,28,29],variou:[18,27],veloc:28,verbos:28,view:27,virtual:29,visualis:28,wa:28,wai:29,we:[27,28,29],well:23,where:[27,29],which:[8,14,22,23,27,28,29],wrap:27,wrapper:23,written:27,x:28,y:28},titles:["bellman","bellman.agents","bellman.agents.background_planning","bellman.agents.decision_time_planning","bellman.agents.mbpo","bellman.agents.mepo","bellman.agents.pets","bellman.agents.trpo","bellman.benchmark","bellman.benchmark.mbpo","bellman.benchmark.mepo","bellman.benchmark.pets","bellman.benchmark.trpo","bellman.distributions","bellman.drivers","bellman.environments","bellman.environments.mixins","bellman.environments.mixins.BatchSizeUpdaterMixin","bellman.environments.transition_model","bellman.environments.transition_model.keras_model","bellman.harness","bellman.networks","bellman.policies","bellman.training","bellman.training.utils","bellman.trajectory_optimisers","Bellman documentation","Approximating MDPs","Learning from samples","Trajectory Optimisation"],titleterms:{agent:[1,2,3,4,5,6,7,28],approxim:27,background_plan:2,batchsizeupdatermixin:17,bellman:[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26],benchmark:[8,9,10,11,12],decision_time_plan:3,detail:29,distribut:13,document:26,driver:14,dynam:28,environ:[15,16,17,18,19,27],from:28,get:26,har:20,implement:29,keras_model:19,learn:28,mathemat:29,mbpo:[4,9],mdp:27,mepo:[5,10],mixin:[16,17],model:[27,28],network:21,optimis:29,pet:[6,11],polici:22,sampl:28,start:26,tf:28,train:[23,24,28],trajectori:29,trajectory_optimis:25,transit:27,transition_model:[18,19],trpo:[7,12],util:24}})