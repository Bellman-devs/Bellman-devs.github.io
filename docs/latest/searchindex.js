Search.setIndex({docnames:["_autosummary/bellman","_autosummary/bellman.agents","_autosummary/bellman.agents.background_planning","_autosummary/bellman.agents.decision_time_planning","_autosummary/bellman.agents.mbpo","_autosummary/bellman.agents.mepo","_autosummary/bellman.agents.pets","_autosummary/bellman.agents.trpo","_autosummary/bellman.benchmark","_autosummary/bellman.benchmark.mepo","_autosummary/bellman.benchmark.pets","_autosummary/bellman.benchmark.trpo","_autosummary/bellman.distributions","_autosummary/bellman.drivers","_autosummary/bellman.environments","_autosummary/bellman.environments.mixins","_autosummary/bellman.environments.mixins.BatchSizeUpdaterMixin","_autosummary/bellman.environments.transition_model","_autosummary/bellman.environments.transition_model.keras_model","_autosummary/bellman.harness","_autosummary/bellman.networks","_autosummary/bellman.policies","_autosummary/bellman.training","_autosummary/bellman.training.utils","_autosummary/bellman.trajectory_optimisers","index","notebooks/approximate_mdps","notebooks/model_visualisation","notebooks/trajectory_optimisation"],envversion:{"sphinx.domains.c":2,"sphinx.domains.changeset":1,"sphinx.domains.citation":1,"sphinx.domains.cpp":3,"sphinx.domains.index":1,"sphinx.domains.javascript":2,"sphinx.domains.math":2,"sphinx.domains.python":2,"sphinx.domains.rst":2,"sphinx.domains.std":2,"sphinx.ext.intersphinx":1,"sphinx.ext.viewcode":1,nbsphinx:3,sphinx:56},filenames:["_autosummary/bellman.rst","_autosummary/bellman.agents.rst","_autosummary/bellman.agents.background_planning.rst","_autosummary/bellman.agents.decision_time_planning.rst","_autosummary/bellman.agents.mbpo.rst","_autosummary/bellman.agents.mepo.rst","_autosummary/bellman.agents.pets.rst","_autosummary/bellman.agents.trpo.rst","_autosummary/bellman.benchmark.rst","_autosummary/bellman.benchmark.mepo.rst","_autosummary/bellman.benchmark.pets.rst","_autosummary/bellman.benchmark.trpo.rst","_autosummary/bellman.distributions.rst","_autosummary/bellman.drivers.rst","_autosummary/bellman.environments.rst","_autosummary/bellman.environments.mixins.rst","_autosummary/bellman.environments.mixins.BatchSizeUpdaterMixin.rst","_autosummary/bellman.environments.transition_model.rst","_autosummary/bellman.environments.transition_model.keras_model.rst","_autosummary/bellman.harness.rst","_autosummary/bellman.networks.rst","_autosummary/bellman.policies.rst","_autosummary/bellman.training.rst","_autosummary/bellman.training.utils.rst","_autosummary/bellman.trajectory_optimisers.rst","index.rst","notebooks/approximate_mdps.ipynb","notebooks/model_visualisation.ipynb","notebooks/trajectory_optimisation.ipynb"],objects:{"":{bellman:[0,0,0,"-"]},"bellman.agents":{background_planning:[2,0,0,"-"],decision_time_planning:[3,0,0,"-"],mbpo:[4,0,0,"-"],mepo:[5,0,0,"-"],pets:[6,0,0,"-"],trpo:[7,0,0,"-"]},"bellman.benchmark":{mepo:[9,0,0,"-"],pets:[10,0,0,"-"],trpo:[11,0,0,"-"]},"bellman.environments":{mixins:[15,0,0,"-"],transition_model:[17,0,0,"-"]},"bellman.environments.mixins":{BatchSizeUpdaterMixin:[16,1,1,""]},"bellman.environments.mixins.BatchSizeUpdaterMixin":{update_batch_size:[16,2,1,""]},"bellman.environments.transition_model":{keras_model:[18,0,0,"-"]},"bellman.training":{utils:[23,0,0,"-"]},bellman:{agents:[1,0,0,"-"],benchmark:[8,0,0,"-"],distributions:[12,0,0,"-"],drivers:[13,0,0,"-"],environments:[14,0,0,"-"],harness:[19,0,0,"-"],networks:[20,0,0,"-"],policies:[21,0,0,"-"],training:[22,0,0,"-"],trajectory_optimisers:[24,0,0,"-"]}},objnames:{"0":["py","module","Python module"],"1":["py","class","Python class"],"2":["py","method","Python method"]},objtypes:{"0":"py:module","1":"py:class","2":"py:method"},terms:{"100":27,"1000":27,"10000":27,"1000000":27,"200":27,"25000":27,"256":27,"3d76e308ab48":27,"423a7fdd97a0":27,"5000":27,"6f4d0f4ab99c":27,"abstract":16,"case":26,"class":[15,16,17,22,26,28],"function":[8,12,26,27],"import":27,"int":16,"new":16,"return":[16,27],"true":26,For:[14,24,27],That:25,The:[0,13,22,25,26,27,28],There:28,These:[27,28],Using:27,a_i:28,a_t:26,abc:16,about:26,action:[13,26,27,28],action_spec:27,adamoptim:27,add_batch:27,after:27,again:27,against:26,agent:[0,8,13,19,22,23,25,26,28],agenttrain:22,aim:0,algorithm:[26,28],all:[26,27],along:27,also:26,altern:25,api:[0,25],appli:28,applic:26,approxim:[14,25,28],as_dataset:27,assembl:27,associ:27,axi:27,b66d7bc1297b:27,background:2,base:[1,16,21,22,25,26,27,28],batch:[16,27],batch_siz:[16,27],becaus:27,been:[8,26,27,28],befor:27,bellman:[26,27],between:26,blue:27,bound:27,build:26,call:[13,26,27],callback:27,can:[26,27,28],capabl:25,captur:27,car:27,cdot:26,check:0,choos:[26,28],chosen:26,classic_control:27,clear:27,collect:27,collect_data_spec:27,collect_driv:27,collect_model_training_episod:27,collect_polici:27,collect_steps_per_iter:27,colour:27,combin:28,common:[26,27],compat:27,compon:[15,16,22,26],compos:[26,28],computation:26,conceiv:25,condit:[27,28],consid:26,consist:[22,26,27],constantreward:27,constrain:28,contain:[17,18,20,21,27],control:27,cross:28,cumul:26,current:28,d9c7c421d8da:27,data:[22,27],data_spec:27,dataset:27,debug_summari:27,decis:[3,25,26,27,28],deep:26,def:27,defin:[13,14,15,22,24,26,27],definit:26,defint:26,demonstr:27,describ:26,design:26,detail:[14,26],determinist:26,differ:[26,28],directli:26,discount:26,discret:27,distribut:[26,27],document:[0,26],done:26,dqn:27,dqn_agent:27,dqnagent:27,driver:27,durat:27,dynam:26,dynamic_episode_driv:27,dynamic_step_driv:27,dynamicepisodedriv:27,dynamicstepdriv:27,e229c71fe371:27,e995a7410889:27,e9c35f5ee664:27,each:[22,26,28],earlystop:27,easili:26,eb5a6cbc97f:27,effici:26,element:27,element_wise_squared_loss:27,enabl:25,end:26,enough:27,ensembl:26,ensur:26,entir:26,entropi:28,enumer:22,env:26,environ:[13,22,27,28],environment_model:27,environmentmodel:[26,27],episod:[26,27],epoch:27,epsilon:27,epsilon_greedi:27,estim:28,eval_polici:27,exampl:[22,27],execut:28,expect:26,expens:26,experi:[8,19,27],extend:25,extens:28,f0acc5f6a430:27,f8e6995a4b48:27,f_i:26,factor:26,fals:27,fc_layer_param:27,flexibl:[26,28],follow:26,form:26,framework:[26,28],free:[22,25],from:[13,22,26,28],fulfil:26,gamma:[26,27,28],gather:27,gather_al:27,gaussian:26,gener:[26,27],get:0,get_initial_st:27,get_or_create_global_step:27,given:28,global_step:27,going:27,gradient_clip:27,greedi:27,green:27,gym:[26,27],hand:26,handl:26,has:[22,26,27],hat:[26,28],have:[8,22,26,27,28],helper:[8,22],horizon:26,how:[26,27],howev:26,hyper:28,implement:[8,17,18,20,22,26],improv:27,impul:27,impuls:27,incorpor:26,independ:28,infinit:26,infinitehorizontrajectorysampl:27,inform:[24,27],infti:26,inifit:26,initi:[25,26,27,28],initial_state_distribut:27,inlin:27,input:27,instead:26,interact:13,interfac:[17,22,26,27],intern:16,involv:26,ipython:27,iter:[27,28],jupyt:25,kera:[18,20,27],keras_model:27,kerastrainingspec:27,kerastransitionmodel:27,kernel:26,last:27,layer:20,learn:[1,21,25,26,28],learning_r:27,left:27,length:[27,28],level:0,librari:[25,26],linear:27,linear_transition_network:27,linearis:27,lineartransitionnetwork:27,list:22,load:27,locat:27,loss:27,made:27,mai:[22,26,28],manag:13,manipul:12,manner:[26,27],mapsto:26,markov:26,mathbb:26,mathcal:[26,28],matplotlib:27,max_length:27,mbrl:25,mdp:[14,25,28],mean:27,method:[16,22,24,28],mirror:0,model:[1,14,15,16,17,18,20,21,22,25,28],model_collect_driv:27,model_training_buff:27,model_training_buffer_capac:27,modul:[2,3,15,17,18,27],modular:26,modulenotfounderror:27,monitor:27,more:[14,22,24,26],most:[26,27],mountain:27,mountain_car:27,mountaincar:27,mountaincarinitialst:27,mountaincarreward:27,mountaincartermin:27,must:26,n_step_upd:27,name:[22,27],nameerror:27,necessari:26,network:[18,26,27],neural:[18,26,27],next:27,next_observ:27,none:[16,27],note:26,notebook:[14,24,25,27],now:27,num_episod:27,num_parallel_cal:27,num_step:27,number:[27,28],numpi:27,object:[22,26,28],observ:27,observation_spec:27,often:26,one:[22,26,27],open:25,openai:26,optim:[27,28],optimis:24,order:27,other:26,out:0,overview:25,packag:[0,1,8,12,13,14,19,20,21,22,24,28],paramet:[16,22,28],parameteris:[22,28],parametris:27,part:[16,25],particular:26,patienc:27,plan:[2,3,28],plot:27,plot_mountain_car_policy_decis:27,plot_mountain_car_transit:27,polici:[13,22,27,28],policy_st:27,policytrajectoryoptimis:28,posit:27,posterior:26,predict:[25,27],prefetch:27,previou:27,probabilist:26,probabl:12,problem:[25,28],process:[26,28],produc:[27,28],provid:[1,8,12,19,22,26,28],python:[25,26],q_net:27,q_network:27,qnetwork:27,random:[27,28],rang:27,read:25,real:[14,22,27,28],recent:27,red:27,refer:[0,25],reinforc:[1,21,25,26,27],repeat:[26,28],repeatedli:28,replac:26,replay_buff:27,replay_buffer_capac:27,repositori:26,repres:27,resampl:26,respect:26,respons:13,reward:[26,27,28],reward_model:27,reward_scale_factor:27,rho_0:[26,28],right:27,role:26,rollout:[26,27,28],run:[8,19,27],s_0:28,s_i:28,s_t:26,same:[26,27],sampl:[13,26,28],sample_batch_s:27,sample_trajectori:27,sample_transit:27,sample_uniformly_distributed_observations_and_get_act:27,sample_uniformly_distributed_transit:27,schedul:22,see:[14,24],sequenc:28,sequenti:25,set:[22,28],shoot:28,should:22,sim:26,singl:[26,27,28],size:16,small:27,solut:28,solv:[25,26,28],some:28,sourc:[16,25],space:[26,27],specif:21,specifi:22,start:[0,26,28],state:[16,26,27,28],statesampl:26,step:[13,26,28],stochast:26,straightforward:27,strategi:26,structur:0,subclass:26,subsequ:27,suite_gym:27,sum:28,summarize_grads_and_var:27,superclass:26,support:[23,26],swap:26,system:[26,27],tackl:25,target_update_period:27,target_update_tau:27,td_errors_loss_fn:27,tensorflow:[25,26,27],termin:[26,27],test:27,tf_agent:27,tf_env:27,tf_py_environ:27,tf_uniform_replay_buff:27,tf_wrapper:27,tfpyenviron:27,tftimelimit:27,tfuniformreplaybuff:27,than:22,thi:[1,8,12,13,14,15,17,18,19,20,21,22,24,26,27,28],those:[22,28],three:27,time:[3,26,28],time_step:27,time_step_spec:27,tool:27,toolbox:[0,8,22,25,26,28],top:0,traceback:27,train_loss:27,train_model_step:27,train_sequence_length:27,train_step:27,train_step_count:27,trainabl:22,training_batch_s:27,training_spec:27,trajectori:[24,26,27],trajectory_optimis:28,trajectory_sampl:27,trajectory_sampling_strategi:27,trajectory_spec:27,transit:[17,18,20,22,27,28],transition_model:27,transitionmodel:26,transtion:26,trial:26,ts1:26,tupl:26,two:[26,27,28],type:[16,26],uniformli:26,unrol:28,until:26,untrain:27,updat:22,update_batch_s:16,use:[16,26,27,28],used:[22,26,27],uses:[26,27],using:[18,20,26,27,28],usual:26,util:27,valu:[16,27,28],variou:[17,26],veloc:27,verbos:27,view:26,virtual:28,visualis:27,wai:28,well:22,where:[26,28],which:[8,13,21,22,26,27,28],wrap:26,wrapper:22,written:26},titles:["bellman","bellman.agents","bellman.agents.background_planning","bellman.agents.decision_time_planning","bellman.agents.mbpo","bellman.agents.mepo","bellman.agents.pets","bellman.agents.trpo","bellman.benchmark","bellman.benchmark.mepo","bellman.benchmark.pets","bellman.benchmark.trpo","bellman.distributions","bellman.drivers","bellman.environments","bellman.environments.mixins","bellman.environments.mixins.BatchSizeUpdaterMixin","bellman.environments.transition_model","bellman.environments.transition_model.keras_model","bellman.harness","bellman.networks","bellman.policies","bellman.training","bellman.training.utils","bellman.trajectory_optimisers","Bellman documentation","Approximating MDPs","Learning from samples","Trajectory Optimisation"],titleterms:{agent:[1,2,3,4,5,6,7,27],approxim:26,background_plan:2,batchsizeupdatermixin:16,bellman:[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],benchmark:[8,9,10,11],decision_time_plan:3,detail:28,distribut:12,document:25,driver:13,dynam:27,environ:[14,15,16,17,18,26],from:27,get:25,har:19,implement:28,keras_model:18,learn:27,mathemat:28,mbpo:4,mdp:26,mepo:[5,9],mixin:[15,16],model:[26,27],network:20,optimis:28,pet:[6,10],polici:21,sampl:27,start:25,train:[22,23,27],trajectori:28,trajectory_optimis:24,transit:26,transition_model:[17,18],trpo:[7,11],util:23}})