

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Learning from samples &mdash; Bellman 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/color_theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Trajectory Optimisation" href="trajectory_optimisation.html" />
    <link rel="prev" title="Approximating MDPs" href="approximate_mdps.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Bellman
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="approximate_mdps.html">Approximating MDPs</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Learning from samples</a></li>
<li class="toctree-l1"><a class="reference internal" href="trajectory_optimisation.html">Trajectory Optimisation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../_autosummary/bellman.html">bellman</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Bellman</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Learning from samples</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Learning-from-samples">
<h1>Learning from samples<a class="headerlink" href="#Learning-from-samples" title="Permalink to this headline">¶</a></h1>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tf_agents.agents.dqn</span> <span class="kn">import</span> <span class="n">dqn_agent</span>
<span class="kn">from</span> <span class="nn">tf_agents.drivers</span> <span class="kn">import</span> <span class="n">dynamic_episode_driver</span><span class="p">,</span> <span class="n">dynamic_step_driver</span>
<span class="kn">from</span> <span class="nn">tf_agents.environments</span> <span class="kn">import</span> <span class="n">suite_gym</span>
<span class="kn">from</span> <span class="nn">tf_agents.environments.tf_py_environment</span> <span class="kn">import</span> <span class="n">TFPyEnvironment</span>
<span class="kn">from</span> <span class="nn">tf_agents.networks</span> <span class="kn">import</span> <span class="n">q_network</span>
<span class="kn">from</span> <span class="nn">tf_agents.replay_buffers</span> <span class="kn">import</span> <span class="n">tf_uniform_replay_buffer</span>
<span class="kn">from</span> <span class="nn">tf_agents.utils</span> <span class="kn">import</span> <span class="n">common</span>

<span class="kn">from</span> <span class="nn">bellman.environments.environment_model</span> <span class="kn">import</span> <span class="n">EnvironmentModel</span>
<span class="kn">from</span> <span class="nn">bellman.environments.tf_wrappers</span> <span class="kn">import</span> <span class="n">TFTimeLimit</span>
<span class="kn">from</span> <span class="nn">bellman.environments.transition_model.keras_model.keras</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">KerasTrainingSpec</span><span class="p">,</span>
    <span class="n">KerasTransitionModel</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">bellman.environments.transition_model.keras_model.linear</span> <span class="kn">import</span> <span class="n">LinearTransitionNetwork</span>
<span class="kn">from</span> <span class="nn">bellman.environments.transition_model.keras_model.trajectory_sampling</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">InfiniteHorizonTrajectorySampling</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">examples.utils.classic_control</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">MountainCarInitialState</span><span class="p">,</span>
    <span class="n">MountainCarReward</span><span class="p">,</span>
    <span class="n">MountainCarTermination</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">examples.utils.mountain_car</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">plot_mountain_car_policy_decisions</span><span class="p">,</span>
    <span class="n">plot_mountain_car_transitions</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">examples.utils.policies</span> <span class="kn">import</span> <span class="n">sample_uniformly_distributed_observations_and_get_actions</span>
<span class="kn">from</span> <span class="nn">examples.utils.trajectories</span> <span class="kn">import</span> <span class="n">sample_uniformly_distributed_transitions</span>
<span class="kn">from</span> <span class="nn">tests.tools.bellman.environments.reward_model</span> <span class="kn">import</span> <span class="n">ConstantReward</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">ModuleNotFoundError</span>                       Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/3602052060.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span><span class="ansi-green-fg">import</span> tensorflow <span class="ansi-green-fg">as</span> tf
<span class="ansi-green-intense-fg ansi-bold">      2</span> <span class="ansi-green-fg">from</span> tf_agents<span class="ansi-blue-fg">.</span>agents<span class="ansi-blue-fg">.</span>dqn <span class="ansi-green-fg">import</span> dqn_agent
<span class="ansi-green-intense-fg ansi-bold">      3</span> <span class="ansi-green-fg">from</span> tf_agents<span class="ansi-blue-fg">.</span>drivers <span class="ansi-green-fg">import</span> dynamic_episode_driver<span class="ansi-blue-fg">,</span> dynamic_step_driver
<span class="ansi-green-intense-fg ansi-bold">      4</span> <span class="ansi-green-fg">from</span> tf_agents<span class="ansi-blue-fg">.</span>environments <span class="ansi-green-fg">import</span> suite_gym
<span class="ansi-green-intense-fg ansi-bold">      5</span> <span class="ansi-green-fg">from</span> tf_agents<span class="ansi-blue-fg">.</span>environments<span class="ansi-blue-fg">.</span>tf_py_environment <span class="ansi-green-fg">import</span> TFPyEnvironment

<span class="ansi-red-fg">ModuleNotFoundError</span>: No module named &#39;tensorflow&#39;
</pre></div></div>
</div>
<p>“”” This notebook demonstrates how to assemble a batch model based reinforcement learning system. In this example we generate trajectories from the Mountain Car gym environment using a random policy, use that data to train a linear model of the transition function, and use sampled transitions from the model to train a DQN agent. “””</p>
<p>%matplotlib inline</p>
<p>This example is based on Mountain Car because it is straightforward to visualise the state space and the linearised dynamics contain enough information to learn a controller using reinforcement learning.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_or_create_global_step</span><span class="p">()</span>
<span class="n">tf_env</span> <span class="o">=</span> <span class="n">TFPyEnvironment</span><span class="p">(</span><span class="n">suite_gym</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;MountainCar-v0&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/2824442390.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>global_step <span class="ansi-blue-fg">=</span> tf<span class="ansi-blue-fg">.</span>compat<span class="ansi-blue-fg">.</span>v1<span class="ansi-blue-fg">.</span>train<span class="ansi-blue-fg">.</span>get_or_create_global_step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> tf_env <span class="ansi-blue-fg">=</span> TFPyEnvironment<span class="ansi-blue-fg">(</span>suite_gym<span class="ansi-blue-fg">.</span>load<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">&#34;MountainCar-v0&#34;</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;tf&#39; is not defined
</pre></div></div>
</div>
<div class="section" id="Dynamics-model">
<h2>Dynamics model<a class="headerlink" href="#Dynamics-model" title="Permalink to this headline">¶</a></h2>
<p>We define the linear model that is going to be used to model the transition function of this environment. We plot the predicted dynamics of the untrained model. The action space of the mountain car environment consists of three discrete elements. These are represented in all subsequent plots with three colours: - left impulse: blue - no impule: red - right impulse: green</p>
<p>In these state-space plots, the x-axis is the agents position and the y-axis is the velocity.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">training_spec</span> <span class="o">=</span> <span class="n">KerasTrainingSpec</span><span class="p">(</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
    <span class="n">training_batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">)],</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">linear_transition_network</span> <span class="o">=</span> <span class="n">LinearTransitionNetwork</span><span class="p">(</span><span class="n">tf_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">())</span>
<span class="n">trajectory_sampling_strategy</span> <span class="o">=</span> <span class="n">InfiniteHorizonTrajectorySampling</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">transition_model</span> <span class="o">=</span> <span class="n">KerasTransitionModel</span><span class="p">(</span>
    <span class="p">[</span><span class="n">linear_transition_network</span><span class="p">],</span>
    <span class="n">tf_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">(),</span>
    <span class="n">tf_env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">(),</span>
<span class="p">)</span>
<span class="n">reward_model</span> <span class="o">=</span> <span class="n">ConstantReward</span><span class="p">(</span><span class="n">tf_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">(),</span> <span class="n">tf_env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">())</span>
<span class="n">sample_transitions</span> <span class="o">=</span> <span class="n">sample_uniformly_distributed_transitions</span><span class="p">(</span>
    <span class="n">transition_model</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">reward_model</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/3286094786.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> batch_size <span class="ansi-blue-fg">=</span> <span class="ansi-cyan-fg">64</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span>
<span class="ansi-green-fg">----&gt; 3</span><span class="ansi-red-fg"> training_spec = KerasTrainingSpec(
</span><span class="ansi-green-intense-fg ansi-bold">      4</span>     epochs<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">5000</span><span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span>     training_batch_size<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">256</span><span class="ansi-blue-fg">,</span>

<span class="ansi-red-fg">NameError</span>: name &#39;KerasTrainingSpec&#39; is not defined
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plot_mountain_car_transitions</span><span class="p">(</span>
    <span class="n">sample_transitions</span><span class="o">.</span><span class="n">observation</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
    <span class="n">sample_transitions</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
    <span class="n">sample_transitions</span><span class="o">.</span><span class="n">next_observation</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/3989627936.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> plot_mountain_car_transitions(
</span><span class="ansi-green-intense-fg ansi-bold">      2</span>     sample_transitions<span class="ansi-blue-fg">.</span>observation<span class="ansi-blue-fg">.</span>numpy<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>     sample_transitions<span class="ansi-blue-fg">.</span>action<span class="ansi-blue-fg">.</span>numpy<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>     sample_transitions<span class="ansi-blue-fg">.</span>next_observation<span class="ansi-blue-fg">.</span>numpy<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> )

<span class="ansi-red-fg">NameError</span>: name &#39;plot_mountain_car_transitions&#39; is not defined
</pre></div></div>
</div>
</div>
<div class="section" id="TF-Agents-Agent">
<h2>TF-Agents Agent<a class="headerlink" href="#TF-Agents-Agent" title="Permalink to this headline">¶</a></h2>
<p>In this notebook we train a TF-Agents DQN agent on samples from the dynamics model. The TF-Agents agents define two policies: a collect policy and a training policy. For this DQN agent, the training policy is a greedy policy parametrised by a Q value neural network, and the collect policy is the associated epsilon greedy policy.</p>
<p>We use the collect policy from the untrained DQN agent to generate trajectories from the real Mountain Car environment in order to train the dynamics model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">train_sequence_length</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">fc_layer_params</span> <span class="o">=</span> <span class="p">(</span><span class="mi">100</span><span class="p">,)</span>
<span class="n">collect_model_training_episodes</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">model_training_buffer_capacity</span> <span class="o">=</span> <span class="mi">25000</span>
<span class="n">collect_steps_per_iteration</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">epsilon_greedy</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">replay_buffer_capacity</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">target_update_tau</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">target_update_period</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">n_step_update</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">gradient_clipping</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">reward_scale_factor</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">debug_summaries</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">summarize_grads_and_vars</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">q_net</span> <span class="o">=</span> <span class="n">q_network</span><span class="o">.</span><span class="n">QNetwork</span><span class="p">(</span>
    <span class="n">tf_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">(),</span> <span class="n">tf_env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">(),</span> <span class="n">fc_layer_params</span><span class="o">=</span><span class="n">fc_layer_params</span>
<span class="p">)</span>


<span class="n">tf_agent</span> <span class="o">=</span> <span class="n">dqn_agent</span><span class="o">.</span><span class="n">DqnAgent</span><span class="p">(</span>
    <span class="n">tf_env</span><span class="o">.</span><span class="n">time_step_spec</span><span class="p">(),</span>
    <span class="n">tf_env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">(),</span>
    <span class="n">q_network</span><span class="o">=</span><span class="n">q_net</span><span class="p">,</span>
    <span class="n">epsilon_greedy</span><span class="o">=</span><span class="n">epsilon_greedy</span><span class="p">,</span>
    <span class="n">n_step_update</span><span class="o">=</span><span class="n">n_step_update</span><span class="p">,</span>
    <span class="n">target_update_tau</span><span class="o">=</span><span class="n">target_update_tau</span><span class="p">,</span>
    <span class="n">target_update_period</span><span class="o">=</span><span class="n">target_update_period</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">),</span>
    <span class="n">td_errors_loss_fn</span><span class="o">=</span><span class="n">common</span><span class="o">.</span><span class="n">element_wise_squared_loss</span><span class="p">,</span>
    <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span>
    <span class="n">reward_scale_factor</span><span class="o">=</span><span class="n">reward_scale_factor</span><span class="p">,</span>
    <span class="n">gradient_clipping</span><span class="o">=</span><span class="n">gradient_clipping</span><span class="p">,</span>
    <span class="n">debug_summaries</span><span class="o">=</span><span class="n">debug_summaries</span><span class="p">,</span>
    <span class="n">summarize_grads_and_vars</span><span class="o">=</span><span class="n">summarize_grads_and_vars</span><span class="p">,</span>
    <span class="n">train_step_counter</span><span class="o">=</span><span class="n">global_step</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">tf_agent</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>


<span class="n">eval_policy</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">policy</span>
<span class="n">collect_policy</span> <span class="o">=</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">collect_policy</span>

<span class="n">model_training_buffer</span> <span class="o">=</span> <span class="n">tf_uniform_replay_buffer</span><span class="o">.</span><span class="n">TFUniformReplayBuffer</span><span class="p">(</span>
    <span class="n">collect_policy</span><span class="o">.</span><span class="n">trajectory_spec</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">tf_env</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">model_training_buffer_capacity</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model_collect_driver</span> <span class="o">=</span> <span class="n">dynamic_episode_driver</span><span class="o">.</span><span class="n">DynamicEpisodeDriver</span><span class="p">(</span>
    <span class="n">tf_env</span><span class="p">,</span>
    <span class="n">collect_policy</span><span class="p">,</span>
    <span class="n">observers</span><span class="o">=</span><span class="p">[</span><span class="n">model_training_buffer</span><span class="o">.</span><span class="n">add_batch</span><span class="p">],</span>
    <span class="n">num_episodes</span><span class="o">=</span><span class="n">collect_model_training_episodes</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model_collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">train_model_step</span><span class="p">():</span>
    <span class="n">trajectories</span> <span class="o">=</span> <span class="n">model_training_buffer</span><span class="o">.</span><span class="n">gather_all</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">transition_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">training_spec</span><span class="p">)</span>


<span class="n">train_model_step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/4200779551.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">     16</span> summarize_grads_and_vars <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">False</span>
<span class="ansi-green-intense-fg ansi-bold">     17</span>
<span class="ansi-green-fg">---&gt; 18</span><span class="ansi-red-fg"> q_net = q_network.QNetwork(
</span><span class="ansi-green-intense-fg ansi-bold">     19</span>     tf_env<span class="ansi-blue-fg">.</span>observation_spec<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> tf_env<span class="ansi-blue-fg">.</span>action_spec<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> fc_layer_params<span class="ansi-blue-fg">=</span>fc_layer_params
<span class="ansi-green-intense-fg ansi-bold">     20</span> )

<span class="ansi-red-fg">NameError</span>: name &#39;q_network&#39; is not defined
</pre></div></div>
</div>
<p>This plot was produced in the same manner as the previous one, but the model has now been trained on data from the real environment. The dynamics of the environment have been captured by the model.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">sample_transitions</span> <span class="o">=</span> <span class="n">sample_uniformly_distributed_transitions</span><span class="p">(</span>
    <span class="n">transition_model</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">reward_model</span>
<span class="p">)</span>

<span class="n">plot_mountain_car_transitions</span><span class="p">(</span>
    <span class="n">sample_transitions</span><span class="o">.</span><span class="n">observation</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
    <span class="n">sample_transitions</span><span class="o">.</span><span class="n">action</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
    <span class="n">sample_transitions</span><span class="o">.</span><span class="n">next_observation</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/4204429102.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> sample_transitions = sample_uniformly_distributed_transitions(
</span><span class="ansi-green-intense-fg ansi-bold">      2</span>     transition_model<span class="ansi-blue-fg">,</span> <span class="ansi-cyan-fg">1000</span><span class="ansi-blue-fg">,</span> reward_model
<span class="ansi-green-intense-fg ansi-bold">      3</span> )
<span class="ansi-green-intense-fg ansi-bold">      4</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> plot_mountain_car_transitions(

<span class="ansi-red-fg">NameError</span>: name &#39;sample_uniformly_distributed_transitions&#39; is not defined
</pre></div></div>
</div>
</div>
<div class="section" id="Training-on-samples">
<h2>Training on samples<a class="headerlink" href="#Training-on-samples" title="Permalink to this headline">¶</a></h2>
<p>We define an environment which uses the trained transition model for the dynamics, along with a reward function, episode termination condition, initial state distributions and bound on episode length.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">reward</span> <span class="o">=</span> <span class="n">MountainCarReward</span><span class="p">(</span><span class="n">tf_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">(),</span> <span class="n">tf_env</span><span class="o">.</span><span class="n">action_spec</span><span class="p">())</span>
<span class="n">terminates</span> <span class="o">=</span> <span class="n">MountainCarTermination</span><span class="p">(</span><span class="n">tf_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">())</span>
<span class="n">initial_state_distribution</span> <span class="o">=</span> <span class="n">MountainCarInitialState</span><span class="p">(</span><span class="n">tf_env</span><span class="o">.</span><span class="n">observation_spec</span><span class="p">())</span>
<span class="n">environment_model</span> <span class="o">=</span> <span class="n">TFTimeLimit</span><span class="p">(</span>
    <span class="n">EnvironmentModel</span><span class="p">(</span><span class="n">transition_model</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">terminates</span><span class="p">,</span> <span class="n">initial_state_distribution</span><span class="p">),</span>
    <span class="n">duration</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/2607481859.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>reward <span class="ansi-blue-fg">=</span> MountainCarReward<span class="ansi-blue-fg">(</span>tf_env<span class="ansi-blue-fg">.</span>observation_spec<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span> tf_env<span class="ansi-blue-fg">.</span>action_spec<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      2</span> terminates <span class="ansi-blue-fg">=</span> MountainCarTermination<span class="ansi-blue-fg">(</span>tf_env<span class="ansi-blue-fg">.</span>observation_spec<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> initial_state_distribution <span class="ansi-blue-fg">=</span> MountainCarInitialState<span class="ansi-blue-fg">(</span>tf_env<span class="ansi-blue-fg">.</span>observation_spec<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span> environment_model = TFTimeLimit(
<span class="ansi-green-intense-fg ansi-bold">      5</span>     EnvironmentModel<span class="ansi-blue-fg">(</span>transition_model<span class="ansi-blue-fg">,</span> reward<span class="ansi-blue-fg">,</span> terminates<span class="ansi-blue-fg">,</span> initial_state_distribution<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">,</span>

<span class="ansi-red-fg">NameError</span>: name &#39;MountainCarReward&#39; is not defined
</pre></div></div>
</div>
<p>The agent is trained on data gathered from the environment model. Using the environment interface means the TF-Agents drivers can be used to generate rollouts.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">tf_uniform_replay_buffer</span><span class="o">.</span><span class="n">TFUniformReplayBuffer</span><span class="p">(</span>
    <span class="n">data_spec</span><span class="o">=</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">collect_data_spec</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">tf_env</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">replay_buffer_capacity</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">collect_driver</span> <span class="o">=</span> <span class="n">dynamic_step_driver</span><span class="o">.</span><span class="n">DynamicStepDriver</span><span class="p">(</span>
    <span class="n">environment_model</span><span class="p">,</span>
    <span class="n">collect_policy</span><span class="p">,</span>
    <span class="n">observers</span><span class="o">=</span><span class="p">[</span><span class="n">replay_buffer</span><span class="o">.</span><span class="n">add_batch</span><span class="p">],</span>
    <span class="n">num_steps</span><span class="o">=</span><span class="n">collect_steps_per_iteration</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/3903815623.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
</span><span class="ansi-green-intense-fg ansi-bold">      2</span>     data_spec<span class="ansi-blue-fg">=</span>tf_agent<span class="ansi-blue-fg">.</span>collect_data_spec<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span>     batch_size<span class="ansi-blue-fg">=</span>tf_env<span class="ansi-blue-fg">.</span>batch_size<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>     max_length<span class="ansi-blue-fg">=</span>replay_buffer_capacity<span class="ansi-blue-fg">,</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> )

<span class="ansi-red-fg">NameError</span>: name &#39;tf_uniform_replay_buffer&#39; is not defined
</pre></div></div>
</div>
<p>Before training the policy we plot the decisions made at random locations in the state space.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">time_step</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">policy_state</span> <span class="o">=</span> <span class="n">collect_policy</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">(</span><span class="n">tf_env</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">time_step</span><span class="o">=</span><span class="n">time_step</span><span class="p">,</span> <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">)</span>

<span class="n">sample_trajectories</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">gather_all</span><span class="p">()</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">sample_uniformly_distributed_observations_and_get_actions</span><span class="p">(</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">policy</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plot_mountain_car_policy_decisions</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s2">&quot;observations&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;actions&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/493832024.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> time_step <span class="ansi-blue-fg">=</span> <span class="ansi-green-fg">None</span>
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg"> </span>policy_state <span class="ansi-blue-fg">=</span> collect_policy<span class="ansi-blue-fg">.</span>get_initial_state<span class="ansi-blue-fg">(</span>tf_env<span class="ansi-blue-fg">.</span>batch_size<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      3</span> collect_driver<span class="ansi-blue-fg">.</span>run<span class="ansi-blue-fg">(</span>time_step<span class="ansi-blue-fg">=</span>time_step<span class="ansi-blue-fg">,</span> policy_state<span class="ansi-blue-fg">=</span>policy_state<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">      4</span>
<span class="ansi-green-intense-fg ansi-bold">      5</span> sample_trajectories <span class="ansi-blue-fg">=</span> replay_buffer<span class="ansi-blue-fg">.</span>gather_all<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;collect_policy&#39; is not defined
</pre></div></div>
</div>
<p>After a single training iteration, we again plot the decisions made by the policy.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">train_step</span><span class="p">():</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">replay_buffer</span><span class="o">.</span><span class="n">as_dataset</span><span class="p">(</span>
        <span class="n">num_parallel_calls</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sample_batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="n">train_sequence_length</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="p">)</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">experience</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">iterator</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf_agent</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>


<span class="n">time_step</span><span class="p">,</span> <span class="n">policy_state</span> <span class="o">=</span> <span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">time_step</span><span class="o">=</span><span class="n">time_step</span><span class="p">,</span> <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">)</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">()</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">sample_uniformly_distributed_observations_and_get_actions</span><span class="p">(</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">policy</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plot_mountain_car_policy_decisions</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s2">&quot;observations&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;actions&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/1814195236.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      8</span>
<span class="ansi-green-intense-fg ansi-bold">      9</span>
<span class="ansi-green-fg">---&gt; 10</span><span class="ansi-red-fg"> </span>time_step<span class="ansi-blue-fg">,</span> policy_state <span class="ansi-blue-fg">=</span> collect_driver<span class="ansi-blue-fg">.</span>run<span class="ansi-blue-fg">(</span>time_step<span class="ansi-blue-fg">=</span>time_step<span class="ansi-blue-fg">,</span> policy_state<span class="ansi-blue-fg">=</span>policy_state<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     11</span> train_loss <span class="ansi-blue-fg">=</span> train_step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     12</span>

<span class="ansi-red-fg">NameError</span>: name &#39;collect_driver&#39; is not defined
</pre></div></div>
</div>
<p>Having trained the policy for a small number of iterations it is clear that the policy is improving.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">time_step</span><span class="p">,</span> <span class="n">policy_state</span> <span class="o">=</span> <span class="n">collect_driver</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">time_step</span><span class="o">=</span><span class="n">time_step</span><span class="p">,</span> <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span>
    <span class="p">)</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">()</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">sample_uniformly_distributed_observations_and_get_actions</span><span class="p">(</span><span class="n">tf_agent</span><span class="o">.</span><span class="n">policy</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plot_mountain_car_policy_decisions</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="s2">&quot;observations&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;actions&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">NameError</span>                                 Traceback (most recent call last)
<span class="ansi-green-fg">/tmp/ipykernel_2478/258358337.py</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-intense-fg ansi-bold">      1</span> <span class="ansi-green-fg">for</span> _ <span class="ansi-green-fg">in</span> range<span class="ansi-blue-fg">(</span><span class="ansi-cyan-fg">5</span><span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">----&gt; 2</span><span class="ansi-red-fg">     time_step, policy_state = collect_driver.run(
</span><span class="ansi-green-intense-fg ansi-bold">      3</span>         time_step<span class="ansi-blue-fg">=</span>time_step<span class="ansi-blue-fg">,</span> policy_state<span class="ansi-blue-fg">=</span>policy_state
<span class="ansi-green-intense-fg ansi-bold">      4</span>     )
<span class="ansi-green-intense-fg ansi-bold">      5</span>     train_loss <span class="ansi-blue-fg">=</span> train_step<span class="ansi-blue-fg">(</span><span class="ansi-blue-fg">)</span>

<span class="ansi-red-fg">NameError</span>: name &#39;collect_driver&#39; is not defined
</pre></div></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="trajectory_optimisation.html" class="btn btn-neutral float-right" title="Trajectory Optimisation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="approximate_mdps.html" class="btn btn-neutral float-left" title="Approximating MDPs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, The Bellman Contributors.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>