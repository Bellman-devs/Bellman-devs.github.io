

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Approximating MDPs &mdash; Bellman 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/color_theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "document", "processHtmlClass": "math|output_area"}}</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Learning from samples" href="model_visualisation.html" />
    <link rel="prev" title="Bellman documentation" href="../index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Bellman
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Approximating MDPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_visualisation.html">Learning from samples</a></li>
<li class="toctree-l1"><a class="reference internal" href="trajectory_optimisation.html">Trajectory Optimisation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../_autosummary/bellman.html">bellman</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Bellman</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Approximating MDPs</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="Approximating-MDPs">
<h1>Approximating MDPs<a class="headerlink" href="#Approximating-MDPs" title="Permalink to this headline">¶</a></h1>
<p>Model based reinforcement learning involves learning a model to approximate the true Markov decision process (MDP), and incorporating that learned model into an algorithm to solve the true MDP. In this document we will describe how this can be done using the Bellman toolbox.</p>
<p>We define an MDP by the following tuple:</p>
<div class="math notranslate nohighlight">
\[\mathcal{M} = (S, A, r, P, \rho_0, \gamma),\]</div>
<p>where <span class="math notranslate nohighlight">\(S\)</span> is the state space, <span class="math notranslate nohighlight">\(A\)</span> is the action space, <span class="math notranslate nohighlight">\(r: S \times A \times S \mapsto \mathbb{R}\)</span> is the reward function, <span class="math notranslate nohighlight">\(P: S \times A \mapsto S\)</span> is the transition kernel, <span class="math notranslate nohighlight">\(\rho_0\)</span> is the initial state distribution, and <span class="math notranslate nohighlight">\(\gamma\)</span> is the discount factor of the discounted cumulative reward objective. The transition kernel <span class="math notranslate nohighlight">\(P\)</span> may be a deterministic function, where <span class="math notranslate nohighlight">\(s_{t+1} = f(s_t, a_t)\)</span>, or may be stochastic, where
<span class="math notranslate nohighlight">\(s_{t+1} \sim P(\cdot | s_t, a_t)\)</span>.</p>
<p>The Bellman toolbox provides a framework for building approximate MDPs with the following form:</p>
<div class="math notranslate nohighlight">
\[\hat{\mathcal{M}} = (S, A, \hat{r}, \hat{P}, \rho_0, \gamma)\]</div>
<p>Note that the difference between <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> and <span class="math notranslate nohighlight">\(\hat{\mathcal{M}}\)</span> is that the true transtion dynamics <span class="math notranslate nohighlight">\(P\)</span> and the true reward function <span class="math notranslate nohighlight">\(r\)</span> have been replaced with the approximate dynamics <span class="math notranslate nohighlight">\(\hat{P}\)</span> and approximate reward function <span class="math notranslate nohighlight">\(\hat{r}\)</span>. However we expect that, for most applications, it is not necessary to learn a reward function but instead use the true reward function <span class="math notranslate nohighlight">\(r\)</span>.</p>
<div class="section" id="Environment-Models">
<h2>Environment Models<a class="headerlink" href="#Environment-Models" title="Permalink to this headline">¶</a></h2>
<p>A common interface which is used for MDPs is the OpenAI Gym <code class="docutils literal notranslate"><span class="pre">Env</span></code> interface. In particular, the TF-Agents library expects MDPs to be implemented against this interface. TF-Agents supports Python environments, but also provides a superclass for environments which are entirely written in TensorFlow.</p>
<p>The Bellman toolbox provides a framework for wrapping approximate transition kernels in an approximate MDP which uses this TF-Agents superclass. This design ensures that the approximate MDP object can be used in the same manner as a “true” MDP, and all of the TF-Agents framework for building systems can be re-used.</p>
<p>The Bellman toolbox has a modular design, so that approximate MDP objects are “composed” of other objects. This design provides the flexibility to easily swap components of the MDP. The <code class="docutils literal notranslate"><span class="pre">EnvironmentModel</span></code> class, which defines the approximate MDP, is composed of the following: * a transition kernel (<span class="math notranslate nohighlight">\(P\)</span>), which must be a subclass of the <code class="docutils literal notranslate"><span class="pre">TransitionModel</span></code> class; * a reward function (<span class="math notranslate nohighlight">\(r\)</span>), which must be a subclass of the <code class="docutils literal notranslate"><span class="pre">Reward</span></code> class; * a termination function, which must be a
subclass of the <code class="docutils literal notranslate"><span class="pre">Termination</span></code> class; * the initial state distribution (<span class="math notranslate nohighlight">\(\rho_0\)</span>), which must be a subclass of the <code class="docutils literal notranslate"><span class="pre">StateSampler</span></code> class.</p>
</div>
<div class="section" id="Transition-Models">
<h2>Transition Models<a class="headerlink" href="#Transition-Models" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">TransitionModel</span></code> interface fulfils the role of the approximate transition kernel <span class="math notranslate nohighlight">\(\hat{P}\)</span> in the defintion of the approximate MDP. Implementations of this interface must provide the definition of the function</p>
<div class="math notranslate nohighlight">
\[s_{t+1} = f(s_t, a_t).\]</div>
<p>In this repository we consider various different types of transition kernels: * ensembles of one or more deterministic neural networks * ensembles of one or more probabilistic neural networks * gaussian processes</p>
<p>A gaussian process transition kernel can be used directly (which can be computationally expensive), or the gaussian process can be approximated in the function space view by an ensemble of functions, in which case the gaussian process can be handled in the same manner as an ensemble of neural networks, c.f. <a class="reference external" href="https://arxiv.org/abs/2002.09309">Efficiently sampling functions from Gaussian process posteriors</a>.</p>
<p>Each of these transition kernels can be used to define a single function <span class="math notranslate nohighlight">\(f\)</span> or an ensemble of functions <span class="math notranslate nohighlight">\(\{f_i\}\)</span>. In the single function case, rollouts are generated by repeated application of the function. In the function ensemble case, a trajectory sampling strategy must be chosen to generate rollouts. In this repository we consider two different trajectory sampling strategies: * single step horizon * infinite horizon</p>
<p>The two strategies are called <span class="math notranslate nohighlight">\(TS1\)</span> and <span class="math notranslate nohighlight">\(TS\infty\)</span> respectively in <a class="reference external" href="https://arxiv.org/abs/1805.12114">Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models</a>, which has more details about these strategies.</p>
<p>The difference is how often to resample the function <span class="math notranslate nohighlight">\(f_i\)</span> to use to generate the rollout. In the single step case, at each time step, choose a function uniformly from the ensemble <span class="math notranslate nohighlight">\(\{f_i\}\)</span>. In the inifite horizon case, choose a function uniformly from the ensemble at the start of the trial, and use that function consistently until the end of the trial (usually one episode).</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="model_visualisation.html" class="btn btn-neutral float-right" title="Learning from samples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../index.html" class="btn btn-neutral float-left" title="Bellman documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, The Bellman Contributors.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>